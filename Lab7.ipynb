{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Heart Attack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "* Read in data\n",
    "* Clean data\n",
    "* Summarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Fitting Models\n",
    "\n",
    "This section asks you to create a final best model for each of the model types studied this week. For each, you should:\n",
    "\n",
    "Find the best model based on ROC AUC for predicting the target variable.\n",
    "\n",
    "Report the (cross-validated!) ROC AUC metric.\n",
    "\n",
    "Fit the final model.\n",
    "\n",
    "Output a confusion matrix; that is, the counts of how many observations fell into each predicted class for each true class.\n",
    "\n",
    "(Where applicable) Interpret the coefficients and/or estimates produced by the model fit.\n",
    "\n",
    "You should certainly try multiple model pipelines to find the best model. You do not need to include the output for every attempted model, but you should describe all of the models explored. You should include any hyperparameter tuning steps in your writeup as well.\n",
    "\n",
    "Q1: KNN\n",
    "Q2: Logistic Regression\n",
    "Q3: Decision Tree\n",
    "Q4: Interpretation\n",
    "Which predictors were most important to predicting heart attack risk?\n",
    "\n",
    "Q5: ROC Curve\n",
    "Plot the ROC Curve for your three models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Metrics\n",
    "\n",
    "Consider the following metrics:\n",
    "\n",
    "True Positive Rate or Recall or Sensitivity = Of the observations that are truly Class A, how many were predicted to be Class A?\n",
    "\n",
    "Precision or Positive Predictive Value = Of all the observations classified as Class A, how many of them were truly from Class A?\n",
    "\n",
    "True Negative Rate or Specificity or Negative Predictive Value = Of all the observations classified as NOT Class A, how many were truly NOT Class A?\n",
    "\n",
    "Compute each of these metrics (cross-validated) for your three models (KNN, Logistic Regression, and Decision Tree) in Part One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Discussion\n",
    "\n",
    "Suppose you have been hired by a hospital to create classification models for heart attack risk.\n",
    "\n",
    "The following questions give a possible scenario for why the hospital is interested in these models. For each one, discuss:\n",
    "\n",
    "Which metric(s) you would use for model selection and why.\n",
    "\n",
    "Which of your final models (Part One Q1-3) you would recommend to the hospital, and why.\n",
    "\n",
    "What score you should expect for your chosen metric(s) using your chosen model to predict future observations.\n",
    "\n",
    "Q1\n",
    "The hospital faces severe lawsuits if they deem a patient to be low risk, and that patient later experiences a heart attack.\n",
    "\n",
    "Q2\n",
    "The hospital is overfull, and wants to only use bed space for patients most in need of monitoring due to heart attack risk.\n",
    "\n",
    "Q3\n",
    "The hospital is studying root causes of heart attacks, and would like to understand which biological measures are associated with heart attack risk.\n",
    "\n",
    "Q4\n",
    "The hospital is training a new batch of doctors, and they would like to compare the diagnoses of these doctors to the predictions given by the algorithm to measure the ability of new doctors to diagnose patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four: Validation\n",
    "\n",
    "Before sharing the dataset with you, I set aside a random 10% of the observations to serve as a final validation set.\n",
    "\n",
    "ha_validation = pd.read_csv(\"https://www.dropbox.com/s/jkwqdiyx6o6oad0/heart_attack_validation.csv?dl=1\")\n",
    "Use each of your final models in Part One Q1-3, predict the target variable in the validation dataset.\n",
    "\n",
    "For each, output a confusion matrix, and report the ROC AUC, the precision, and the recall.\n",
    "\n",
    "Compare these values to the cross-validated estimates you reported in Part One and Part Two. Did our measure of model success turn out to be approximately correct for the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Five: Cohen's Kappa\n",
    "\n",
    "Another common metric used in classification is Cohen’s Kappa.\n",
    "\n",
    "Use online resources to research this measurement. Calculate it for the models from Part One, Q1-3, and discuss reasons or scenarios that would make us prefer to use this metric as our measure of model success. Do your conclusions from above change if you judge your models using Cohen’s Kappa instead? Does this make sense?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
