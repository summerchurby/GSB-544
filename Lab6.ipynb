{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Variable Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Different Model Specs\n",
    "### A. Regression without regularization\n",
    "1. Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, make_scorer, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = pd.read_csv(r\"C:\\Users\\achur\\OneDrive\\Desktop\\School\\CP Fall 2024\\544\\Hitters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "def regression_analysis(X, y, model_type = \"linear\", alpha_values = [0.001, 0.01, 0.1, 1, 10], l1_ratio_values = [0.0, 0.25, 0.5, 0.75, 1.0], cv = 5):\n",
    "    if model_type == \"linear\":\n",
    "        model = LinearRegression()\n",
    "        alpha = {}\n",
    "        l1_ratio = {}\n",
    "    elif model_type == \"ridge\":\n",
    "        model = Ridge()\n",
    "        alpha = {\"regression__alpha\": alpha_values}\n",
    "        l1_ratio = {}\n",
    "    elif model_type == \"lasso\":\n",
    "        model = Lasso()\n",
    "        alpha = {\"regression__alpha\": alpha_values}\n",
    "        l1_ratio = {}\n",
    "    elif model_type == \"elastic_net\":\n",
    "        model = ElasticNet()\n",
    "        alpha = {\"regression__alpha\": alpha_values, \"regression__l1_ratio\": l1_ratio_values}\n",
    "    else:\n",
    "        raise ValueError(\"Error\")\n",
    "\n",
    "    # pipeline\n",
    "    ct = ColumnTransformer([(\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
    "                                        make_column_selector(dtype_include=object)),\n",
    "                            (\"standardize\", StandardScaler(),\n",
    "                                        make_column_selector(dtype_include=np.number))],\n",
    "                                        remainder = \"passthrough\")\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessing\", ct),\n",
    "        (\"regression\", model)\n",
    "    ])\n",
    "\n",
    "    # grid search\n",
    "    grid_search = GridSearchCV(pipeline, alpha, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    grid_search_fitted = grid_search.fit(X, y)\n",
    "\n",
    "    # best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model_fitted = best_model.fit(X, y)\n",
    "\n",
    "    # coefficients\n",
    "    coefs = best_model.named_steps[\"regression\"].coef_\n",
    "    feature_names = best_model_fitted.named_steps[\"preprocessing\"].get_feature_names_out()\n",
    "\n",
    "    # data frame\n",
    "    coefs_df = pd.DataFrame({\n",
    "        \"Feature Name\": feature_names,\n",
    "        \"Coefficients\": coefs})\n",
    "\n",
    "    print(\"Cross-validated MSE scores:\", -grid_search_fitted.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "    # best model values\n",
    "    best_alpha = grid_search.best_params_.get(\"regression__alpha\", None)\n",
    "    best_l1_ratio = grid_search.best_params_.get(\"regression__l1_ratio\", None)\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # print best alpha and l1 ratio\n",
    "    if best_alpha is not None:\n",
    "        print(f\"Best alpha: {best_alpha}\")\n",
    "    if best_l1_ratio is not None:\n",
    "        print(f\"Best l1 ratio: {best_l1_ratio}\")\n",
    "    print(f\"Best cross-validated MSE score: {-best_score}\")\n",
    "\n",
    "    return coefs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit this pipeline to the full dataset, and interpret a few of the most important coefficients.\n",
    "3. Use cross-validation to estimate the MSE you would expect if you used this pipeline to predict 1989 salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Linear Regression -----\n",
      "----- Linear Regression -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated MSE scores: [120548.42849608]\n",
      "Best cross-validated MSE score: 120548.42849608236\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A    -31.299712\n",
      "1      dummify__League_N     31.299712\n",
      "2    dummify__Division_E     58.424623\n",
      "3    dummify__Division_W    -58.424623\n",
      "4   dummify__NewLeague_A     12.381163\n",
      "5   dummify__NewLeague_N    -12.381163\n",
      "6     standardize__AtBat   -291.094556\n",
      "7      standardize__Hits    337.830479\n",
      "8     standardize__HmRun     37.853837\n",
      "9      standardize__Runs    -60.572479\n",
      "10      standardize__RBI    -26.994984\n",
      "11    standardize__Walks    135.073897\n",
      "12    standardize__Years    -16.693359\n",
      "13   standardize__CAtBat   -391.038655\n",
      "14    standardize__CHits     86.687617\n",
      "15   standardize__CHmRun    -14.181723\n",
      "16    standardize__CRuns    480.747135\n",
      "17     standardize__CRBI    260.689886\n",
      "18   standardize__CWalks   -213.892259\n",
      "19  standardize__PutOuts     78.761296\n",
      "20  standardize__Assists     53.732490\n",
      "21   standardize__Errors    -22.160862\n",
      "Cross-validated MSE scores: [120548.42849608]\n",
      "Best cross-validated MSE score: 120548.42849608236\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A    -31.299712\n",
      "1      dummify__League_N     31.299712\n",
      "2    dummify__Division_E     58.424623\n",
      "3    dummify__Division_W    -58.424623\n",
      "4   dummify__NewLeague_A     12.381163\n",
      "5   dummify__NewLeague_N    -12.381163\n",
      "6     standardize__AtBat   -291.094556\n",
      "7      standardize__Hits    337.830479\n",
      "8     standardize__HmRun     37.853837\n",
      "9      standardize__Runs    -60.572479\n",
      "10      standardize__RBI    -26.994984\n",
      "11    standardize__Walks    135.073897\n",
      "12    standardize__Years    -16.693359\n",
      "13   standardize__CAtBat   -391.038655\n",
      "14    standardize__CHits     86.687617\n",
      "15   standardize__CHmRun    -14.181723\n",
      "16    standardize__CRuns    480.747135\n",
      "17     standardize__CRBI    260.689886\n",
      "18   standardize__CWalks   -213.892259\n",
      "19  standardize__PutOuts     78.761296\n",
      "20  standardize__Assists     53.732490\n",
      "21   standardize__Errors    -22.160862\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Linear Regression -----\")\n",
    "coefs_linear = regression_analysis(X, y, model_type=\"linear\")\n",
    "print(coefs_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of coefficients: The variable CRuns represents the relationship between total number of runs throughout the career and salary. This means that with each additional career run, the predicted salary is expected to increase by 480.74 units. The variable Hits represents the relationship between the number of hits in a season and salary. This means that with each additional hit in a season, the predicted salary is expected to increase by 337.83 units. The variable CRBI represents the relationship between total number of runs batted in in a career and salary. This means that with every additional RBI in a career, salary is expected to increase by 260.689 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Ridge regression\n",
    "1. Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary ridge regression.\n",
    "2. Use cross-validation to tune the lambda hyperparameter.\n",
    "3. Fit the pipeline with your chosen lambda to the full dataset, and interpret a few of the most important coefficients.\n",
    "4. Report the MSE you would expect if you used this pipeline to predict 1989 salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Ridge Regression -----\n",
      "\n",
      "\n",
      "----- Ridge Regression -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated MSE scores: [121124.45859214 121022.90328584 120343.62106698 119144.43267692\n",
      " 119348.9847757 ]\n",
      "Best alpha: 1\n",
      "Best cross-validated MSE score: 119144.4326769158\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A    -30.438855\n",
      "1      dummify__League_N     30.438855\n",
      "2    dummify__Division_E     60.015595\n",
      "3    dummify__Division_W    -60.015595\n",
      "4   dummify__NewLeague_A     13.111282\n",
      "5   dummify__NewLeague_N    -13.111282\n",
      "6     standardize__AtBat   -270.686441\n",
      "7      standardize__Hits    296.645050\n",
      "8     standardize__HmRun     18.100592\n",
      "9      standardize__Runs    -29.339406\n",
      "10      standardize__RBI     -9.113295\n",
      "11    standardize__Walks    124.407173\n",
      "12    standardize__Years    -38.667748\n",
      "13   standardize__CAtBat   -225.406548\n",
      "14    standardize__CHits    126.659607\n",
      "15   standardize__CHmRun     39.070924\n",
      "16    standardize__CRuns    320.412169\n",
      "17     standardize__CRBI    160.386784\n",
      "18   standardize__CWalks   -184.423611\n",
      "19  standardize__PutOuts     78.623656\n",
      "20  standardize__Assists     47.462597\n",
      "21   standardize__Errors    -23.724190\n",
      "Cross-validated MSE scores: [121124.45859214 121022.90328584 120343.62106698 119144.43267692\n",
      " 119348.9847757 ]\n",
      "Best alpha: 1\n",
      "Best cross-validated MSE score: 119144.4326769158\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A    -30.438855\n",
      "1      dummify__League_N     30.438855\n",
      "2    dummify__Division_E     60.015595\n",
      "3    dummify__Division_W    -60.015595\n",
      "4   dummify__NewLeague_A     13.111282\n",
      "5   dummify__NewLeague_N    -13.111282\n",
      "6     standardize__AtBat   -270.686441\n",
      "7      standardize__Hits    296.645050\n",
      "8     standardize__HmRun     18.100592\n",
      "9      standardize__Runs    -29.339406\n",
      "10      standardize__RBI     -9.113295\n",
      "11    standardize__Walks    124.407173\n",
      "12    standardize__Years    -38.667748\n",
      "13   standardize__CAtBat   -225.406548\n",
      "14    standardize__CHits    126.659607\n",
      "15   standardize__CHmRun     39.070924\n",
      "16    standardize__CRuns    320.412169\n",
      "17     standardize__CRBI    160.386784\n",
      "18   standardize__CWalks   -184.423611\n",
      "19  standardize__PutOuts     78.623656\n",
      "20  standardize__Assists     47.462597\n",
      "21   standardize__Errors    -23.724190\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression (with alpha hyperparameter tuning)\n",
    "print(\"\\n----- Ridge Regression -----\")\n",
    "coefs_ridge = regression_analysis(X, y, model_type=\"ridge\")\n",
    "print(coefs_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of coefficients: The variable CRuns represents the relationship between total number of runs throughout the career and salary. This means that with each additional career run, the predicted salary is expected to increase by 320.41 units. The variable Hits represents the relationship between the number of hits in a season and salary. This means that with each additional hit in a season, the predicted salary is expected to increase by 296.645 units. The variable CRBI represents the relationship between total number of runs batted in in a career and salary. This means that with every additional RBI in a career, salary is expected to increase by 160.387 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Lasso Regression\n",
    "1. Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary ridge regression.\n",
    "2. Use cross-validation to tune the lambda hyperparameter.\n",
    "3. Fit the pipeline with your chosen lambda to the full dataset, and interpret a few of the most important coefficients.\n",
    "4. Report the MSE you would expect if you used this pipeline to predict 1989 salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Lasso Regression -----\n",
      "\n",
      "\n",
      "----- Lasso Regression -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+07, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.012e+07, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.096e+06, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.537e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+06, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.059e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.591e+06, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.471e+05, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.066e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.923e+04, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+05, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.158e+05, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+04, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.282e+05, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.815e+03, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated MSE scores: [120994.17981481 120964.76468618 120682.25263745 119761.58740741\n",
      " 121828.14133339]\n",
      "Best alpha: 1\n",
      "Best cross-validated MSE score: 119761.58740741303\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A -3.582607e+01\n",
      "1      dummify__League_N  1.734720e-15\n",
      "2    dummify__Division_E  1.144130e+02\n",
      "3    dummify__Division_W -2.191014e-11\n",
      "4   dummify__NewLeague_A  0.000000e+00\n",
      "5   dummify__NewLeague_N -0.000000e+00\n",
      "6     standardize__AtBat -2.823710e+02\n",
      "7      standardize__Hits  3.043595e+02\n",
      "8     standardize__HmRun  1.112702e+01\n",
      "9      standardize__Runs -2.496651e+01\n",
      "10      standardize__RBI -0.000000e+00\n",
      "11    standardize__Walks  1.206953e+02\n",
      "12    standardize__Years -3.494815e+01\n",
      "13   standardize__CAtBat -1.626398e+02\n",
      "14    standardize__CHits  0.000000e+00\n",
      "15   standardize__CHmRun  1.422599e+01\n",
      "16    standardize__CRuns  3.755655e+02\n",
      "17     standardize__CRBI  1.926109e+02\n",
      "18   standardize__CWalks -1.896446e+02\n",
      "19  standardize__PutOuts  7.876037e+01\n",
      "20  standardize__Assists  4.199668e+01\n",
      "21   standardize__Errors -1.847938e+01\n",
      "Cross-validated MSE scores: [120994.17981481 120964.76468618 120682.25263745 119761.58740741\n",
      " 121828.14133339]\n",
      "Best alpha: 1\n",
      "Best cross-validated MSE score: 119761.58740741303\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A -3.582607e+01\n",
      "1      dummify__League_N  1.734720e-15\n",
      "2    dummify__Division_E  1.144130e+02\n",
      "3    dummify__Division_W -2.191014e-11\n",
      "4   dummify__NewLeague_A  0.000000e+00\n",
      "5   dummify__NewLeague_N -0.000000e+00\n",
      "6     standardize__AtBat -2.823710e+02\n",
      "7      standardize__Hits  3.043595e+02\n",
      "8     standardize__HmRun  1.112702e+01\n",
      "9      standardize__Runs -2.496651e+01\n",
      "10      standardize__RBI -0.000000e+00\n",
      "11    standardize__Walks  1.206953e+02\n",
      "12    standardize__Years -3.494815e+01\n",
      "13   standardize__CAtBat -1.626398e+02\n",
      "14    standardize__CHits  0.000000e+00\n",
      "15   standardize__CHmRun  1.422599e+01\n",
      "16    standardize__CRuns  3.755655e+02\n",
      "17     standardize__CRBI  1.926109e+02\n",
      "18   standardize__CWalks -1.896446e+02\n",
      "19  standardize__PutOuts  7.876037e+01\n",
      "20  standardize__Assists  4.199668e+01\n",
      "21   standardize__Errors -1.847938e+01\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression (with alpha hyperparameter tuning)\n",
    "print(\"\\n----- Lasso Regression -----\")\n",
    "coefs_lasso = regression_analysis(X, y, model_type=\"lasso\")\n",
    "print(coefs_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Elastic Net\n",
    "1. Create a pipeline that includes all the columns as predictors for Salary, and performs ordinary ridge regression.\n",
    "2. Use cross-validation to tune the lambda and alpha hyperparameters.\n",
    "3. Fit the pipeline with your chosen hyperparameters to the full dataset, and interpret a few of the most important coefficients.\n",
    "4. Report the MSE you would expect if you used this pipeline to predict 1989 salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Elastic Net Regression -----\n",
      "\n",
      "\n",
      "----- Elastic Net Regression -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+07, tolerance: 4.708e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.229e+06, tolerance: 3.606e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+07, tolerance: 4.137e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.457e+06, tolerance: 4.281e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.787e+06, tolerance: 4.558e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e+07, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.053e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+07, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.242e+06, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.638e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.012e+07, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.995e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+07, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.060e+06, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.579e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+07, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.981e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+07, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.701e+06, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.553e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+07, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.012e+07, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.096e+06, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.537e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.083e+07, tolerance: 4.708e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.466e+06, tolerance: 3.606e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.079e+07, tolerance: 4.137e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.906e+06, tolerance: 4.281e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.026e+07, tolerance: 4.558e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.600e+05, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+04, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+05, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.044e+04, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.069e+04, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+06, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.059e+06, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.591e+06, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.471e+05, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.066e+06, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.215e+07, tolerance: 4.708e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+07, tolerance: 3.606e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+07, tolerance: 4.137e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.040e+06, tolerance: 4.281e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.137e+07, tolerance: 4.558e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.923e+04, tolerance: 4.708e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+05, tolerance: 3.606e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.158e+05, tolerance: 4.137e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+04, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.282e+05, tolerance: 4.558e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.458e+07, tolerance: 4.708e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.147e+07, tolerance: 3.606e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+07, tolerance: 4.137e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.122e+07, tolerance: 4.281e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.356e+07, tolerance: 4.558e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.815e+03, tolerance: 4.281e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+07, tolerance: 4.708e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.509e+07, tolerance: 3.606e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.732e+07, tolerance: 4.137e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.668e+07, tolerance: 4.281e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.858e+07, tolerance: 4.558e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated MSE scores: [119911.32888951 120077.76864311 120296.21077347 120590.44872106\n",
      " 120994.17981481 118957.96789175 119009.79955442 119123.79308625\n",
      " 119404.65600998 120964.76468618 119805.47261377 119636.17022676\n",
      " 119381.37557129 119036.41354271 120682.25263745 122029.76100625\n",
      " 121374.33374621 120775.68540067 120356.59599777 119761.58740741\n",
      " 150034.24612426 144021.73108229 136766.42534889 128407.50504979\n",
      " 121828.14133339]\n",
      "Best alpha: 0.01\n",
      "Best l1 ratio: 0.0\n",
      "Best cross-validated MSE score: 118957.96789174949\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A    -29.055921\n",
      "1      dummify__League_N     29.055922\n",
      "2    dummify__Division_E     60.813166\n",
      "3    dummify__Division_W    -60.813166\n",
      "4   dummify__NewLeague_A     12.395109\n",
      "5   dummify__NewLeague_N    -12.395109\n",
      "6     standardize__AtBat   -233.288530\n",
      "7      standardize__Hits    249.932752\n",
      "8     standardize__HmRun      5.366905\n",
      "9      standardize__Runs     -6.952571\n",
      "10      standardize__RBI      1.909416\n",
      "11    standardize__Walks    111.867856\n",
      "12    standardize__Years    -49.514175\n",
      "13   standardize__CAtBat   -122.140608\n",
      "14    standardize__CHits    123.652433\n",
      "15   standardize__CHmRun     55.646471\n",
      "16    standardize__CRuns    226.828966\n",
      "17     standardize__CRBI    122.933345\n",
      "18   standardize__CWalks   -156.494373\n",
      "19  standardize__PutOuts     77.975918\n",
      "20  standardize__Assists     41.454945\n",
      "21   standardize__Errors    -24.753916\n",
      "Cross-validated MSE scores: [119911.32888951 120077.76864311 120296.21077347 120590.44872106\n",
      " 120994.17981481 118957.96789175 119009.79955442 119123.79308625\n",
      " 119404.65600998 120964.76468618 119805.47261377 119636.17022676\n",
      " 119381.37557129 119036.41354271 120682.25263745 122029.76100625\n",
      " 121374.33374621 120775.68540067 120356.59599777 119761.58740741\n",
      " 150034.24612426 144021.73108229 136766.42534889 128407.50504979\n",
      " 121828.14133339]\n",
      "Best alpha: 0.01\n",
      "Best l1 ratio: 0.0\n",
      "Best cross-validated MSE score: 118957.96789174949\n",
      "            Feature Name  Coefficients\n",
      "0      dummify__League_A    -29.055921\n",
      "1      dummify__League_N     29.055922\n",
      "2    dummify__Division_E     60.813166\n",
      "3    dummify__Division_W    -60.813166\n",
      "4   dummify__NewLeague_A     12.395109\n",
      "5   dummify__NewLeague_N    -12.395109\n",
      "6     standardize__AtBat   -233.288530\n",
      "7      standardize__Hits    249.932752\n",
      "8     standardize__HmRun      5.366905\n",
      "9      standardize__Runs     -6.952571\n",
      "10      standardize__RBI      1.909416\n",
      "11    standardize__Walks    111.867856\n",
      "12    standardize__Years    -49.514175\n",
      "13   standardize__CAtBat   -122.140608\n",
      "14    standardize__CHits    123.652433\n",
      "15   standardize__CHmRun     55.646471\n",
      "16    standardize__CRuns    226.828966\n",
      "17     standardize__CRBI    122.933345\n",
      "18   standardize__CWalks   -156.494373\n",
      "19  standardize__PutOuts     77.975918\n",
      "20  standardize__Assists     41.454945\n",
      "21   standardize__Errors    -24.753916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.265e+07, tolerance: 5.332e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\achur\\Downloads\\python\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.265e+07, tolerance: 5.332e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net Regression (with alpha and l1_ratio hyperparameter tuning)\n",
    "print(\"\\n----- Elastic Net Regression -----\")\n",
    "coefs_elastic_net = regression_analysis(X, y, model_type=\"elastic_net\")\n",
    "print(coefs_elastic_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Variable Selection\n",
    "Based on the above results, decide on:\n",
    "\n",
    "* Which numeric variable is most important.\n",
    "\n",
    "* Which five numeric variables are most important\n",
    "\n",
    "* Which categorical variable is most important\n",
    "\n",
    "For each of the four model specifications, compare the following possible feature sets:\n",
    "\n",
    "1. Using only the one best numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using only the five best variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using the five best numeric variables and their interactions with the one best categorical variable.\n",
    "\n",
    "Report which combination of features and model performed best, based on the validation metric of MSE. (Note: lambda and alpha must be re-tuned for each feature set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Discussion\n",
    "\n",
    "### A. Ridge\n",
    "\n",
    "Compare your Ridge models with your ordinary regression models. How did your coefficients compare? Why does this make sense?\n",
    "\n",
    "### B. Lasso\n",
    "\n",
    "Compare your LASSO model in I with your three LASSO models in II. Did you get the same lambda results? Why does this make sense? Did you get the same MSEs? Why does this make sense?\n",
    "\n",
    "### C. Elastic Net\n",
    "\n",
    "Compare your MSEs for the Elastic Net models with those for the Ridge and LASSO models. Why does it make sense that Elastic Net always “wins”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Final Model\n",
    "\n",
    "Fit your final best pipeline on the full dataset, and summarize your results in a few short sentences and a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix and References"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
